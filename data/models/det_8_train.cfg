[net]
# Testing
# batch=1
# subdivisions=1
# Training
batch=14
subdivisions=1

width=640
height=640
channels=1
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
gaussian_noise = 1
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 400200
policy=steps
steps=300000,350000
scales=.1,.1

[convolutional]
batch_normalize=1
filters=72
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=96
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=96
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=72
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=72
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=96
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=144
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=96
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=96
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=144
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=160
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=144
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=144
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=160
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=320
size=3
stride=1
pad=1
activation=leaky

##################################
# stopbackword=1

[convolutional]
batch_normalize=1
filters=160
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=320
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=39
activation=linear



[yolo]
mask = 6,7,8
anchors=5,30,30,5,30,30,10,60,60,10,60,60,20,120,120,20,120,120
classes=8
num=9
jitter=.3
# scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.01
iou_loss=ciou
ignore_thresh = .8
truth_thresh = 1
random=0
# resize=1.5
# nms_kind=greedynms
# beta_nms=0.6

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=144
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 23

[convolutional]
batch_normalize=1
filters=160
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=39
activation=linear

[yolo]
mask = 3,4,5
anchors=5,30,30,5,30,30,10,60,60,10,60,60,20,120,120,20,120,120
classes=8
num=9
jitter=.3
# scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.01
iou_loss=ciou
ignore_thresh = .8
truth_thresh = 1
random=0
# resize=1.5
# nms_kind=greedynms
# beta_nms=0.6


[route]
layers = -3

[convolutional]
batch_normalize=1
filters=96
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 15

[convolutional]
batch_normalize=1
filters=144
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=39
activation=linear

[yolo]
mask = 0,1,2
anchors=5,30,30,5,30,30,10,60,60,10,60,60,20,120,120,20,120,120
classes=8
num=9
jitter=.3
# scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.01
iou_loss=ciou
ignore_thresh = .8
truth_thresh = 1
random=0
# resize=1.5
# nms_kind=greedynms
# beta_nms=0.6

